{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5675ee7f-ba89-4440-82c1-575408c3c0ab",
   "metadata": {},
   "source": [
    "# Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b97563-77e2-4b95-95ba-15a9db03485a",
   "metadata": {},
   "source": [
    "Ridge Regression, also known as L2 regularization, is a linear regression technique used to mitigate overfitting by adding a penalty term to the standard Ordinary Least Squares (OLS) regression. In OLS, the goal is to minimize the sum of squared residuals between the predicted and actual values. In contrast, Ridge Regression adds a penalty term that discourages large coefficient values:\n",
    "\n",
    "Ridge Loss = OLS Loss + λ * Σ(βi)²\n",
    "\n",
    "OLS minimizes the sum of squared residuals.\n",
    "Ridge minimizes the sum of squared residuals plus a penalty term that discourages large coefficient values.\n",
    "λ (lambda) controls the strength of the penalty; higher λ results in smaller coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42675dea-23e1-4537-ba6c-328044c4d46b",
   "metadata": {},
   "source": [
    "# Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed9a4ce-a1cf-4bd9-8590-795c73c62760",
   "metadata": {},
   "source": [
    "Ridge Regression assumes many of the same assumptions as ordinary linear regression, including linearity, independence of errors, constant variance of errors (homoscedasticity), and normality of errors. Additionally, it assumes that:\n",
    "\n",
    "There is no multicollinearity, or the predictors are not highly correlated.\n",
    "The values of the tuning parameter λ are chosen appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac62d780-f5a1-42b6-a480-a2cda872f367",
   "metadata": {},
   "source": [
    "# Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7957a29-cabc-48e7-ac1b-9ec5b2043398",
   "metadata": {},
   "source": [
    "The value of λ in Ridge Regression is selected through a process called hyperparameter tuning. Common methods include:\n",
    "\n",
    "Cross-Validation: Splitting the data into training and validation sets and selecting the value of λ that minimizes the validation error (e.g., using k-fold cross-validation).\n",
    "Grid Search: Trying a range of λ values and selecting the one with the best performance.\n",
    "Regularization Paths: Algorithms like coordinate descent can automatically search for the optimal λ along a path of possible values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f86000-fed0-477a-9ed2-5f7ff99214de",
   "metadata": {},
   "source": [
    "# Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325cf093-fe49-401c-8c63-76d1d30cecfe",
   "metadata": {},
   "source": [
    "Ridge Regression tends to shrink coefficients toward zero but rarely sets them exactly to zero. While it's not primarily a feature selection method, it can still help in identifying less important features by reducing their impact. If you require strict feature selection, Lasso Regression (L1 regularization) is a better choice because it can set coefficients exactly to zero, effectively excluding features from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040f487e-013e-415d-8394-729fbe7e6412",
   "metadata": {},
   "source": [
    "# Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ab85d6-6a50-437c-9e39-ef2d1b19230a",
   "metadata": {},
   "source": [
    "Ridge Regression is effective at handling multicollinearity (high correlation between predictors) because it distributes the impact of correlated variables more evenly by reducing the magnitude of coefficients. It helps stabilize the model and prevents large coefficient values, making it a useful tool when multicollinearity is a concern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e635f0-afef-4a77-ad54-013b08e8c0da",
   "metadata": {},
   "source": [
    "# Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9332e21b-063c-4106-a79d-8ea97cd7629f",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can handle both categorical and continuous independent variables. However, categorical variables usually need to be converted into a numerical format (e.g., one-hot encoding) before applying Ridge Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0dc40f-b8d7-4d1d-9f2e-577be2068ba3",
   "metadata": {},
   "source": [
    "# Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2903d939-ab55-414f-8427-a3f21676e681",
   "metadata": {},
   "source": [
    "Interpreting Ridge Regression coefficients is similar to interpreting coefficients in ordinary linear regression. Each coefficient represents the change in the dependent variable associated with a one-unit change in the corresponding independent variable while keeping other variables constant. The key difference is that Ridge coefficients are shrunk toward zero, so their magnitude may be smaller than in OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4029163-52c9-4a23-8532-1c9c78be57c2",
   "metadata": {},
   "source": [
    "# Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46ef0e5-c9e6-4fd2-9f0c-eb109dd31b58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
